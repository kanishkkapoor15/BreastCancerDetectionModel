---
title: "cancerDetection"
author: "Kanishk Kapoor"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Load the libraries
library(keras)
library(tensorflow)
library(tidyverse)  # For data wrangling
library(caret)      # For train-test split
library(magick)     # Optional: for image manipulation (alternative to imager)
```
```{r}
library(abind)
```

## Data Loading & Pre-processing


```{r}
raster_to_array <- function(ras) {
  # Convert to matrix
  ras_matrix <- as.matrix(ras)
  
  # Extract RGB from hex (e.g., "#RRGGBB")
  rgb_matrix <- col2rgb(ras_matrix)
  
  # Normalize to 0-1 and reshape to (width, height, 3)
  array_3d <- array(rgb_matrix / 255, dim = c(3, dim(ras_matrix)))
  array_3d <- aperm(array_3d, c(2, 3, 1))  # Reorder to (x, y, channels)
  
  return(array_3d)
}
```

```{r}
dataset_path <- "patientData"

load_images_magick <- function(base_path,max_images_per_class = 200) {
  patient_dirs <- list.dirs(base_path, recursive = FALSE)
  
  images_list <- list()
  labels_list <- c()
  
  for(patient in patient_dirs){
    for(label in c("0","1")) {
      label_dir <- file.path(patient,label)
      
      if(!dir.exists(label_dir)) next
      
      image_files <- list.files(label_dir, pattern ="\\.png$", full.names = TRUE)
      
      image_files <- head(image_files,max_images_per_class)
      
      for(img_path in image_files) {
        
        img <- image_read(img_path) %>%
          image_scale("32x32!") %>%
          as.raster()
        
        img_array <- raster_to_array(img)
        
        images_list[[length(images_list) + 1]] <- img_array
        labels_list <- c(labels_list, as.integer(label))
      }
    }
  }
  
  images_array <- abind::abind(images_list, along=0)
  return(list(images = images_array, labels = labels_list))
}
```

```{r}
# Call the function and store the output
data <- load_images_magick(dataset_path, max_images_per_class = 200)
```

```{r}
# Plot a few random images with labels
library(grid)

# Pick random 5 images
set.seed(123)
sample_ids <- sample(1:95008, 5)

# Create a new page and layout
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 5)))

for (i in 1:5) {
  img <- data$images[sample_ids[i],,,,drop=FALSE]  # keep image shape
  img <- array(img, dim = c(32, 32, 3))  # remove batch dim
  
  pushViewport(viewport(layout.pos.row = 1, layout.pos.col = i))
  grid.raster(img)
  grid.text(label = paste("Label:", data$labels[sample_ids[i]]), y = unit(0.1, "npc"), gp = gpar(col = "white", fontsize = 10))
  popViewport()
}
```
TRAIN AND TEST SETS

```{r}
# Make sure nothing is masked
rm(x_train, x_test, y_train, y_test, train_index)

# Re-run the split
set.seed(123)
train_index <- createDataPartition(data$labels, p = 0.8, list = FALSE)

x_train <- data$images[train_index,,,]
y_train <- data$labels[train_index]

x_test <- data$images[-train_index,,,]
y_test <- data$labels[-train_index]

# Recheck dimensions
dim(x_test)
```

```{r}
dim(x_train)
dim(x_test)
table(y_train)
table(y_test)
```
CNN Model Building

```{r}
use_backend("tensorflow")


#define model

model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size= c(3,3), activation ="relu",
                input_shape = c(32 , 32, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  
  layer_flatten() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")  # Binary output

```

```{r}
# Compile the model
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)

# View model summary
summary(model)
```





```{r}
history <- model %>% fit(
  x = x_train,
  y = y_train,
  epochs = 10,
  batch_size = 64,
  validation_split = 0.2
)
```
The model improved steadily till around epoch 8. That’s when both training and validation accuracy hit their highest point. After that, performance on the validation set started to drop a bit — a sign of potential overfitting.


```{r}
plot(history)
```

```{r}
model %>% evaluate(x_test, y_test)
```
594/594 [==============================] - 3s 6ms/step - loss: 0.4471 - accuracy: 0.7921
     loss  accuracy 
0.4470559 0.7921162 

	•	Test Loss: 0.4471
This is the binary cross-entropy loss on your unseen test data. Lower is better, and this is quite decent considering your training loss plateaued around ~0.40.
	•	Test Accuracy: 79.21%
That means our model correctly predicted the class for ~79% of test samples, which is a solid performance for a binary image classification problem.

```{r}
library(ggplot2)

# Accuracy data
performance_df <- data.frame(
  Dataset = c("Training", "Validation", "Test"),
  Accuracy = c(0.821, 0.775, 0.792)
)

# Bar plot
ggplot(performance_df, aes(x = Dataset, y = Accuracy, fill = Dataset)) +
  geom_bar(stat = "identity", width = 0.5, color = "black") +
  scale_fill_manual(values = c("skyblue", "lightgreen", "salmon")) +
  geom_text(aes(label = round(Accuracy, 3)), vjust = -0.5, size = 5) +
  labs(title = "Model Accuracy Comparison",
       y = "Accuracy",
       x = "") +
  theme_minimal(base_size = 14)
```
Conclusion:

In this project, we developed a Convolutional Neural Network (CNN) to classify images for binary classification of cancer detection using a dataset of medical images. After preprocessing the data and splitting it into training, validation, and test sets, we built a model that successfully learned to differentiate between the two classes (cancerous vs. non-cancerous).

Key Insights:
	1.	Model Architecture:
	•	The CNN architecture included multiple convolutional and max-pooling layers, followed by dense layers and dropout for regularization. This architecture helped the model effectively capture spatial hierarchies and reduce overfitting.
	2.	Model Performance:
	•	On the training dataset, the model achieved an accuracy of 82.1% and on the test dataset, it achieved 79.2% accuracy, showing strong generalization capabilities. The validation accuracy was slightly lower at 77.5%, but still competitive.
	3.	Learning Progress:
	•	The training and validation losses decreased steadily over the course of 10 epochs, indicating the model’s ability to learn effectively from the data.
	•	Early stopping could further improve performance by preventing overfitting and optimizing the model’s accuracy.
	4.	Visualizations:
	•	Accuracy and Loss plots provided a clear view of the training process, helping to visualize the improvement across epochs.
	•	A performance comparison bar chart effectively communicated the model’s evaluation on different datasets, highlighting the model’s robustness.
